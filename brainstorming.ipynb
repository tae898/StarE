{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/stare/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:168: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/tk/.virtualenvs/stare/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:181: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/tk/.virtualenvs/stare/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:127: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method should be an int or np.int64, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "/home/tk/.virtualenvs/stare/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "from typing import Dict\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_scatter\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.init import xavier_normal_\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_add, scatter_max, scatter_mean\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "env = gym.make(\n",
    "    \"room_env:RoomEnv-v2\",\n",
    "    room_size=\"xl-different-prob\",\n",
    "    randomize_observations=\"objects\",\n",
    ")\n",
    "observations, info = env.reset()\n",
    "\n",
    "entities = [e for entities in env.unwrapped.entities.values() for e in entities]\n",
    "relations = env.unwrapped.relations + [\"current_time\", \"timestamp\", \"strength\"]\n",
    "\n",
    "\n",
    "def maybe_num_nodes(index, num_nodes=None):\n",
    "    return index.max().item() + 1 if num_nodes is None else num_nodes\n",
    "\n",
    "\n",
    "def softmax(src, index, num_nodes=None):\n",
    "    r\"\"\"Computes a sparsely evaluated softmax.\n",
    "    Given a value tensor :attr:`src`, this function first groups the values\n",
    "    along the first dimension based on the indices specified in :attr:`index`,\n",
    "    and then proceeds to compute the softmax individually for each group.\n",
    "\n",
    "    Args:\n",
    "        src (Tensor): The source tensor.\n",
    "        index (LongTensor): The indices of elements for applying the softmax.\n",
    "        num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "            :obj:`max_val + 1` of :attr:`index`. (default: :obj:`None`)\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = maybe_num_nodes(index, num_nodes)\n",
    "\n",
    "    out = src - scatter_max(src, index, dim=0, dim_size=num_nodes)[0][index]\n",
    "    out = out.exp()\n",
    "    out = out / (scatter_add(out, index, dim=0, dim_size=num_nodes)[index] + 1e-16)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_param(shape):\n",
    "    param = Parameter(torch.Tensor(*shape))\n",
    "    xavier_normal_(param.data)\n",
    "    return param\n",
    "\n",
    "\n",
    "def com_mult(a, b):\n",
    "    r1, i1 = a[..., 0], a[..., 1]\n",
    "    r2, i2 = b[..., 0], b[..., 1]\n",
    "    return torch.stack([r1 * r2 - i1 * i2, r1 * i2 + i1 * r2], dim=-1)\n",
    "\n",
    "\n",
    "def conj(a):\n",
    "    a[..., 1] = -a[..., 1]\n",
    "    return a\n",
    "\n",
    "\n",
    "def cconv(a, b):\n",
    "    return torch.irfft(\n",
    "        com_mult(torch.rfft(a, 1), torch.rfft(b, 1)), 1, signal_sizes=(a.shape[-1],)\n",
    "    )\n",
    "\n",
    "\n",
    "def ccorr(a, b):\n",
    "    return torch.irfft(\n",
    "        com_mult(conj(torch.rfft(a, 1)), torch.rfft(b, 1)),\n",
    "        1,\n",
    "        signal_sizes=(a.shape[-1],),\n",
    "    )\n",
    "\n",
    "\n",
    "def rotate(h, r):\n",
    "    # re: first half, im: second half\n",
    "    # assume embedding dim is the last dimension\n",
    "    d = h.shape[-1]\n",
    "    h_re, h_im = torch.split(h, d // 2, -1)\n",
    "    r_re, r_im = torch.split(r, d // 2, -1)\n",
    "    return torch.cat([h_re * r_re - h_im * r_im, h_re * r_im + h_im * r_re], dim=-1)\n",
    "\n",
    "\n",
    "def scatter_(name, src, index, dim_size=None):\n",
    "    r\"\"\"Aggregates all values from the :attr:`src` tensor at the indices\n",
    "    specified in the :attr:`index` tensor along the first dimension.\n",
    "    If multiple indices reference the same location, their contributions\n",
    "    are aggregated according to :attr:`name` (either :obj:`\"add\"`,\n",
    "    :obj:`\"mean\"` or :obj:`\"max\"`).\n",
    "\n",
    "    Args:\n",
    "        name (string): The aggregation to use (:obj:`\"add\"`, :obj:`\"mean\"`,\n",
    "            :obj:`\"max\"`).\n",
    "        src (Tensor): The source tensor.\n",
    "        index (LongTensor): The indices of elements to scatter.\n",
    "        dim_size (int, optional): Automatically create output tensor with size\n",
    "            :attr:`dim_size` in the first dimension. If set to :attr:`None`, a\n",
    "            minimal sized output tensor is returned. (default: :obj:`None`)\n",
    "\n",
    "    :rtype: :class:`Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    assert name in [\"add\", \"mean\", \"max\"]\n",
    "\n",
    "    op = getattr(torch_scatter, \"scatter_{}\".format(name))\n",
    "    fill_value = -1e38 if name == \"max\" else 0\n",
    "    out = op(src, index, 0, None, dim_size, fill_value)\n",
    "    if isinstance(out, tuple):\n",
    "        out = out[0]\n",
    "\n",
    "    if name == \"max\":\n",
    "        out[out == fill_value] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class StarEConvLayer(MessagePassing):\n",
    "    \"\"\"The important stuff.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, num_rels, act=lambda x: x, config=None\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(flow=\"target_to_source\", aggr=\"add\")\n",
    "\n",
    "        self.p = config\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_rels = num_rels\n",
    "        self.act = act\n",
    "        self.device = None\n",
    "\n",
    "        self.w_loop = get_param((in_channels, out_channels))  # (100,200)\n",
    "        self.w_in = get_param((in_channels, out_channels))  # (100,200)\n",
    "        self.w_out = get_param((in_channels, out_channels))  # (100,200)\n",
    "        self.w_rel = get_param((in_channels, out_channels))  # (100,200)\n",
    "\n",
    "        if self.p[\"STATEMENT_LEN\"] != 3:\n",
    "            if (\n",
    "                self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"sum\"\n",
    "                or self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"mul\"\n",
    "            ):\n",
    "                self.w_q = get_param((in_channels, in_channels))  # new for quals setup\n",
    "            elif self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"concat\":\n",
    "                self.w_q = get_param(\n",
    "                    (2 * in_channels, in_channels)\n",
    "                )  # need 2x size due to the concat operation\n",
    "\n",
    "        self.loop_rel = get_param((1, in_channels))  # (1,100)\n",
    "        self.loop_ent = get_param((1, in_channels))  # new\n",
    "\n",
    "        self.drop = torch.nn.Dropout(self.p[\"STAREARGS\"][\"GCN_DROP\"])\n",
    "        self.bn = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"ATTENTION\"]:\n",
    "            assert (\n",
    "                self.p[\"STAREARGS\"][\"GCN_DIM\"] == self.p[\"EMBEDDING_DIM\"]\n",
    "            ), \"Current attn implementation requires those tto be identical\"\n",
    "            assert (\n",
    "                self.p[\"EMBEDDING_DIM\"] % self.p[\"STAREARGS\"][\"ATTENTION_HEADS\"] == 0\n",
    "            ), \"should be divisible\"\n",
    "            self.heads = self.p[\"STAREARGS\"][\"ATTENTION_HEADS\"]\n",
    "            self.attn_dim = self.out_channels // self.heads\n",
    "            self.negative_slope = self.p[\"STAREARGS\"][\"ATTENTION_SLOPE\"]\n",
    "            self.attn_drop = self.p[\"STAREARGS\"][\"ATTENTION_DROP\"]\n",
    "            self.att = get_param((1, self.heads, 2 * self.attn_dim))\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"BIAS\"]:\n",
    "            self.register_parameter(\"bias\", Parameter(torch.zeros(out_channels)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        edge_index,\n",
    "        edge_type,\n",
    "        rel_embed,\n",
    "        qualifier_ent=None,\n",
    "        qualifier_rel=None,\n",
    "        quals=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        See end of doc string for explaining.\n",
    "\n",
    "        :param x: all entities*dim_of_entities (for jf17k -> 28646*200)\n",
    "        :param edge_index: COO matrix (2 list each having nodes with index\n",
    "        [1,2,3,4,5]\n",
    "        [3,4,2,5,4]\n",
    "\n",
    "        Here node 1 and node 3 are connected with edge.\n",
    "        And the type of edge can be found using edge_type.\n",
    "\n",
    "        Note that there are twice the number of edges as each edge is also reversed.\n",
    "        )\n",
    "        :param edge_type: The type of edge connecting the COO matrix\n",
    "        :param rel_embed: 2 Times Total relation * emb_dim (200 in our case and 2 Times because of inverse relations)\n",
    "        :param qualifier_ent:\n",
    "        :param qualifier_rel:\n",
    "        :param quals: Another sparse matrix\n",
    "\n",
    "        where\n",
    "            quals[0] --> qualifier relations type\n",
    "            quals[1] --> qualifier entity\n",
    "            quals[2] --> index of the original COO matrix that states for which edge this qualifier exists ()\n",
    "\n",
    "\n",
    "        For argument sake if a knowledge graph has following statements\n",
    "\n",
    "        [e1,p1,e4,qr1,qe1,qr2,qe2]\n",
    "        [e1,p1,e2,qr1,qe1,qr2,qe3]\n",
    "        [e1,p2,e3,qr3,qe3,qr2,qe2]\n",
    "        [e1,p2,e5,qr1,qe1]\n",
    "        [e2,p1,e4]\n",
    "        [e4,p3,e3,qr4,qe1,qr2,qe4]\n",
    "        [e1,p1,e5]\n",
    "                                                 (incoming)         (outgoing)\n",
    "                                            <----(regular)------><---(inverse)------->\n",
    "        Edge index would be             :   [e1,e1,e1,e1,e2,e4,e1,e4,e2,e3,e5,e4,e3,e5]\n",
    "                                            [e4,e2,e3,e5,e4,e3,e5,e1,e1,e1,e1,e2,e4,e1]\n",
    "\n",
    "        Edge Type would be              :   [p1,p1,p2,p2,p1,p3,p1,p1_inv,p1_inv,p2_inv,p2_inv,p1_inv,p3_inv,p1_inv]\n",
    "\n",
    "                                            <-------on incoming-----------------><---------on outgoing-------------->\n",
    "        quals would be                  :   [qr1,qr2,qr1,qr2,qr3,qr2,qr1,qr4,qr2,qr1,qr2,qr1,qr2,qr3,qr2,qr1,qr4,qr2]\n",
    "                                            [qe1,qe2,qe1,qe3,qe3,qe2,qe1,qe1,qe4,qe1,qe2,qe1,qe3,qe3,qe2,qe1,qe1,qe4]\n",
    "                                            [0,0,1,1,2,2,3,5,5,0,0,1,1,2,2,3,5,5]\n",
    "                                            <--on incoming---><--outgoing------->\n",
    "\n",
    "        Note that qr1,qr2... and qe1, qe2, ... all belong to the same space\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.device is None:\n",
    "            self.device = edge_index.device\n",
    "\n",
    "        rel_embed = torch.cat([rel_embed, self.loop_rel], dim=0)\n",
    "        num_edges = edge_index.size(1) // 2\n",
    "        num_ent = x.size(0)\n",
    "\n",
    "        self.in_index, self.out_index = (\n",
    "            edge_index[:, :num_edges],\n",
    "            edge_index[:, num_edges:],\n",
    "        )\n",
    "        self.in_type, self.out_type = edge_type[:num_edges], edge_type[num_edges:]\n",
    "\n",
    "        if self.p[\"STATEMENT_LEN\"] != 3:\n",
    "            num_quals = quals.size(1) // 2\n",
    "            self.in_index_qual_ent, self.out_index_qual_ent = (\n",
    "                quals[1, :num_quals],\n",
    "                quals[1, num_quals:],\n",
    "            )\n",
    "            self.in_index_qual_rel, self.out_index_qual_rel = (\n",
    "                quals[0, :num_quals],\n",
    "                quals[0, num_quals:],\n",
    "            )\n",
    "            self.quals_index_in, self.quals_index_out = (\n",
    "                quals[2, :num_quals],\n",
    "                quals[2, num_quals:],\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "            Adding self loop by creating a COO matrix. Thus \\\n",
    "             loop index [1,2,3,4,5]\n",
    "                        [1,2,3,4,5]\n",
    "             loop type [10,10,10,10,10] --> assuming there are 9 relations\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Self edges between all the nodes\n",
    "        self.loop_index = torch.stack(\n",
    "            [torch.arange(num_ent), torch.arange(num_ent)]\n",
    "        ).to(self.device)\n",
    "        self.loop_type = torch.full(\n",
    "            (num_ent,), rel_embed.size(0) - 1, dtype=torch.long\n",
    "        ).to(\n",
    "            self.device\n",
    "        )  # if rel meb is 500, the index of the self emb is\n",
    "        # 499 .. which is just added here\n",
    "\n",
    "        self.in_norm = self.compute_norm(self.in_index, num_ent)\n",
    "        self.out_norm = self.compute_norm(self.out_index, num_ent)\n",
    "\n",
    "        if self.p[\"STATEMENT_LEN\"] != 3:\n",
    "\n",
    "            in_res = self.propagate(\n",
    "                self.in_index,\n",
    "                x=x,\n",
    "                edge_type=self.in_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=self.in_norm,\n",
    "                mode=\"in\",\n",
    "                ent_embed=x,\n",
    "                qualifier_ent=self.in_index_qual_ent,\n",
    "                qualifier_rel=self.in_index_qual_rel,\n",
    "                qual_index=self.quals_index_in,\n",
    "                source_index=self.in_index[0],\n",
    "            )\n",
    "\n",
    "            loop_res = self.propagate(\n",
    "                self.loop_index,\n",
    "                x=x,\n",
    "                edge_type=self.loop_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=None,\n",
    "                mode=\"loop\",\n",
    "                ent_embed=None,\n",
    "                qualifier_ent=None,\n",
    "                qualifier_rel=None,\n",
    "                qual_index=None,\n",
    "                source_index=None,\n",
    "            )\n",
    "\n",
    "            out_res = self.propagate(\n",
    "                self.out_index,\n",
    "                x=x,\n",
    "                edge_type=self.out_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=self.out_norm,\n",
    "                mode=\"out\",\n",
    "                ent_embed=x,\n",
    "                qualifier_ent=self.out_index_qual_ent,\n",
    "                qualifier_rel=self.out_index_qual_rel,\n",
    "                qual_index=self.quals_index_out,\n",
    "                source_index=self.out_index[0],\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            in_res = self.propagate(\n",
    "                self.in_index,\n",
    "                x=x,\n",
    "                edge_type=self.in_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=self.in_norm,\n",
    "                mode=\"in\",\n",
    "                ent_embed=None,\n",
    "                qualifier_ent=None,\n",
    "                qualifier_rel=None,\n",
    "                qual_index=None,\n",
    "                source_index=None,\n",
    "            )\n",
    "\n",
    "            loop_res = self.propagate(\n",
    "                self.loop_index,\n",
    "                x=x,\n",
    "                edge_type=self.loop_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=None,\n",
    "                mode=\"loop\",\n",
    "                ent_embed=None,\n",
    "                qualifier_ent=None,\n",
    "                qualifier_rel=None,\n",
    "                qual_index=None,\n",
    "                source_index=None,\n",
    "            )\n",
    "\n",
    "            out_res = self.propagate(\n",
    "                self.out_index,\n",
    "                x=x,\n",
    "                edge_type=self.out_type,\n",
    "                rel_embed=rel_embed,\n",
    "                edge_norm=self.out_norm,\n",
    "                mode=\"out\",\n",
    "                ent_embed=None,\n",
    "                qualifier_ent=None,\n",
    "                qualifier_rel=None,\n",
    "                qual_index=None,\n",
    "                source_index=None,\n",
    "            )\n",
    "\n",
    "        out = (\n",
    "            self.drop(in_res) * (1 / 3)\n",
    "            + self.drop(out_res) * (1 / 3)\n",
    "            + loop_res * (1 / 3)\n",
    "        )\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"BIAS\"]:\n",
    "            out = out + self.bias\n",
    "        out = self.bn(out)\n",
    "\n",
    "        # Ignoring the self loop inserted, return.\n",
    "        return self.act(out), torch.matmul(rel_embed, self.w_rel)[:-1]\n",
    "\n",
    "    def rel_transform(self, ent_embed, rel_embed):\n",
    "        if self.p[\"STAREARGS\"][\"OPN\"] == \"corr\":\n",
    "            trans_embed = ccorr(ent_embed, rel_embed)\n",
    "        elif self.p[\"STAREARGS\"][\"OPN\"] == \"sub\":\n",
    "            trans_embed = ent_embed - rel_embed\n",
    "        elif self.p[\"STAREARGS\"][\"OPN\"] == \"mult\":\n",
    "            trans_embed = ent_embed * rel_embed\n",
    "        elif self.p[\"STAREARGS\"][\"OPN\"] == \"rotate\":\n",
    "            trans_embed = rotate(ent_embed, rel_embed)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return trans_embed\n",
    "\n",
    "    def qual_transform(self, qualifier_ent, qualifier_rel):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.p[\"STAREARGS\"][\"QUAL_OPN\"] == \"corr\":\n",
    "            trans_embed = ccorr(qualifier_ent, qualifier_rel)\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_OPN\"] == \"sub\":\n",
    "            trans_embed = qualifier_ent - qualifier_rel\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_OPN\"] == \"mult\":\n",
    "            trans_embed = qualifier_ent * qualifier_rel\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_OPN\"] == \"rotate\":\n",
    "            trans_embed = rotate(qualifier_ent, qualifier_rel)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return trans_embed\n",
    "\n",
    "    def qualifier_aggregate(\n",
    "        self, qualifier_emb, rel_part_emb, alpha=0.5, qual_index=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "            In qualifier_aggregate method following steps are performed\n",
    "\n",
    "            qualifier_emb looks like -\n",
    "            qualifier_emb      :   [a,b,c,d,e,f,g,......]               (here a,b,c ... are of 200 dim)\n",
    "            rel_part_emb       :   [qq,ww,ee,rr,tt, .....]                      (here qq, ww, ee .. are of 200 dim)\n",
    "\n",
    "            Note that rel_part_emb for jf17k would be around 61k*200\n",
    "\n",
    "            Step1 : Pass the qualifier_emb to self.coalesce_quals and multiply the returned output with a weight.\n",
    "            qualifier_emb   : [aa,bb,cc,dd,ee, ...... ]                 (here aa, bb, cc are of 200 dim each)\n",
    "            Note that now qualifier_emb has the same shape as rel_part_emb around 61k*200\n",
    "\n",
    "            Step2 : Combine the updated qualifier_emb (see Step1) with rel_part_emb based on defined aggregation strategy.\n",
    "\n",
    "\n",
    "\n",
    "            Aggregates the qualifier matrix (3, edge_index, emb_dim)\n",
    "        :param qualifier_emb:\n",
    "        :param rel_part_emb:\n",
    "        :param type:\n",
    "        :param alpha\n",
    "        :return:\n",
    "\n",
    "        self.coalesce_quals    returns   :  [q+a+b+d,w+c+e+g,e'+f,......]        (here each element in the list is of 200 dim)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"sum\":\n",
    "            qualifier_emb = torch.einsum(\n",
    "                \"ij,jk -> ik\",\n",
    "                self.coalesce_quals(qualifier_emb, qual_index, rel_part_emb.shape[0]),\n",
    "                self.w_q,\n",
    "            )\n",
    "            return (\n",
    "                alpha * rel_part_emb + (1 - alpha) * qualifier_emb\n",
    "            )  # [N_EDGES / 2 x EMB_DIM]\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"concat\":\n",
    "            qualifier_emb = self.coalesce_quals(\n",
    "                qualifier_emb, qual_index, rel_part_emb.shape[0]\n",
    "            )\n",
    "            agg_rel = torch.cat(\n",
    "                (rel_part_emb, qualifier_emb), dim=1\n",
    "            )  # [N_EDGES / 2 x 2 * EMB_DIM]\n",
    "            return torch.mm(agg_rel, self.w_q)  # [N_EDGES / 2 x EMB_DIM]\n",
    "\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_AGGREGATE\"] == \"mul\":\n",
    "            qualifier_emb = torch.mm(\n",
    "                self.coalesce_quals(\n",
    "                    qualifier_emb, qual_index, rel_part_emb.shape[0], fill=1\n",
    "                ),\n",
    "                self.w_q,\n",
    "            )\n",
    "            return rel_part_emb * qualifier_emb\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def update_rel_emb_with_qualifier(\n",
    "        self,\n",
    "        ent_embed,\n",
    "        rel_embed,\n",
    "        qualifier_ent,\n",
    "        qualifier_rel,\n",
    "        edge_type,\n",
    "        qual_index=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The update_rel_emb_with_qualifier method performs following functions:\n",
    "\n",
    "        Input is the secondary COO matrix (QE (qualifier entity), QR (qualifier relation), edge index (Connection to the primary COO))\n",
    "\n",
    "        Step1 : Embed all the input\n",
    "            Step1a : Embed the qualifier entity via ent_embed (So QE shape is 33k,1 -> 33k,200)\n",
    "            Step1b : Embed the qualifier relation via rel_embed (So QR shape is 33k,1 -> 33k,200)\n",
    "            Step1c : Embed the main statement edge_type via rel_embed (So edge_type shape is 61k,1 -> 61k,200)\n",
    "\n",
    "        Step2 : Combine qualifier entity emb and qualifier relation emb to create qualifier emb (See self.qual_transform).\n",
    "            This is generally just summing up. But can be more any pair-wise function that returns one vector for a (qe,qr) vector\n",
    "\n",
    "        Step3 : Update the edge_type embedding with qualifier information. This uses scatter_add/scatter_mean.\n",
    "\n",
    "\n",
    "        before:\n",
    "            qualifier_emb      :   [a,b,c,d,e,f,g,......]               (here a,b,c ... are of 200 dim)\n",
    "            qual_index         :   [1,1,2,1,2,3,2,......]               (here 1,2,3 .. are edge index of Main COO)\n",
    "            edge_type          :   [q,w,e',r,t,y,u,i,o,p, .....]        (here q,w,e' .. are of 200 dim each)\n",
    "\n",
    "        After:\n",
    "            edge_type          :   [q+(a+b+d),w+(c+e+g),e'+f,......]        (here each element in the list is of 200 dim)\n",
    "\n",
    "\n",
    "        :param ent_embed: essentially x (28k*200 in case of Jf17k)\n",
    "        :param rel_embed: essentially relation embedding matrix\n",
    "\n",
    "        For secondary COO matrix (QE, QR, edge index)\n",
    "        :param qualifier_ent:  QE\n",
    "        :param qualifier_rel: QR\n",
    "        edge_type:\n",
    "        :return:\n",
    "\n",
    "        index select from embedding\n",
    "        phi operation between qual_ent, qual_rel\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: embedding\n",
    "        qualifier_emb_rel = rel_embed[qualifier_rel]\n",
    "        qualifier_emb_ent = ent_embed[qualifier_ent]\n",
    "\n",
    "        rel_part_emb = rel_embed[edge_type]\n",
    "\n",
    "        # Step 2: pass it through qual_transform\n",
    "        qualifier_emb = self.qual_transform(\n",
    "            qualifier_ent=qualifier_emb_ent, qualifier_rel=qualifier_emb_rel\n",
    "        )\n",
    "\n",
    "        # Pass it through a aggregate layer\n",
    "        return self.qualifier_aggregate(\n",
    "            qualifier_emb,\n",
    "            rel_part_emb,\n",
    "            alpha=self.p[\"STAREARGS\"][\"TRIPLE_QUAL_WEIGHT\"],\n",
    "            qual_index=qual_index,\n",
    "        )\n",
    "\n",
    "    # return qualifier_emb\n",
    "    def message(\n",
    "        self,\n",
    "        x_j,\n",
    "        x_i,\n",
    "        edge_type,\n",
    "        rel_embed,\n",
    "        edge_norm,\n",
    "        mode,\n",
    "        ent_embed=None,\n",
    "        qualifier_ent=None,\n",
    "        qualifier_rel=None,\n",
    "        qual_index=None,\n",
    "        source_index=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        The message method performs following functions\n",
    "\n",
    "        Step1 : get updated relation representation (rel_embed) [edge_type] by aggregating qualifier information (self.update_rel_emb_with_qualifier).\n",
    "        Step2 : Obtain edge message by transforming the node embedding with updated relation embedding (self.rel_transform).\n",
    "        Step3 : Multiply edge embeddings (transform) by weight\n",
    "        Step4 : Return the messages. They will be sent to subjects (1st line in the edge index COO)\n",
    "        Over here the node embedding [the first list in COO matrix] is representing the message which will be sent on each edge\n",
    "\n",
    "\n",
    "        More information about updating relation representation please refer to self.update_rel_emb_with_qualifier\n",
    "\n",
    "        :param x_j: objects of the statements (2nd line in the COO)\n",
    "        :param x_i: subjects of the statements (1st line in the COO)\n",
    "        :param edge_type: relation types\n",
    "        :param rel_embed: embedding matrix of all relations\n",
    "        :param edge_norm:\n",
    "        :param mode: in (direct) / out (inverse) / loop\n",
    "        :param ent_embed: embedding matrix of all entities\n",
    "        :param qualifier_ent:\n",
    "        :param qualifier_rel:\n",
    "        :param qual_index:\n",
    "        :param source_index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        weight = getattr(self, \"w_{}\".format(mode))\n",
    "\n",
    "        if self.p[\"STATEMENT_LEN\"] != 3:\n",
    "            # add code here\n",
    "            if mode != \"loop\":\n",
    "                rel_emb = self.update_rel_emb_with_qualifier(\n",
    "                    ent_embed,\n",
    "                    rel_embed,\n",
    "                    qualifier_ent,\n",
    "                    qualifier_rel,\n",
    "                    edge_type,\n",
    "                    qual_index,\n",
    "                )\n",
    "            else:\n",
    "                rel_emb = torch.index_select(rel_embed, 0, edge_type)\n",
    "        else:\n",
    "            rel_emb = torch.index_select(rel_embed, 0, edge_type)\n",
    "\n",
    "        xj_rel = self.rel_transform(x_j, rel_emb)\n",
    "        out = torch.einsum(\"ij,jk->ik\", xj_rel, weight)\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"ATTENTION\"] and mode != \"loop\":\n",
    "            out = out.view(-1, self.heads, self.attn_dim)\n",
    "            x_i = x_i.view(-1, self.heads, self.attn_dim)\n",
    "\n",
    "            alpha = torch.einsum(\n",
    "                \"bij,kij -> bi\", [torch.cat([x_i, out], dim=-1), self.att]\n",
    "            )\n",
    "            alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "            alpha = softmax(alpha, source_index, ent_embed.size(0))\n",
    "            alpha = F.dropout(alpha, p=self.attn_drop)\n",
    "            return out * alpha.view(-1, self.heads, 1)\n",
    "        else:\n",
    "            return out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out, mode):\n",
    "        if self.p[\"STAREARGS\"][\"ATTENTION\"] and mode != \"loop\":\n",
    "            aggr_out = aggr_out.view(-1, self.heads * self.attn_dim)\n",
    "\n",
    "        return aggr_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_norm(edge_index, num_ent):\n",
    "        \"\"\"\n",
    "        Re-normalization trick used by GCN-based architectures without attention.\n",
    "\n",
    "        Yet another torch scatter functionality. See coalesce_quals for a rough idea.\n",
    "\n",
    "        row         :      [1,1,2,3,3,4,4,4,4, .....]        (about 61k for Jf17k)\n",
    "        edge_weight :      [1,1,1,1,1,1,1,1,1,  ....] (same as row. So about 61k for Jf17k)\n",
    "        deg         :      [2,1,2,4,.....]            (same as num_ent about 28k in case of Jf17k)\n",
    "\n",
    "        :param edge_index:\n",
    "        :param num_ent:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        row, col = edge_index\n",
    "        edge_weight = torch.ones_like(\n",
    "            row\n",
    "        ).float()  # Identity matrix where we know all entities are there\n",
    "        deg = scatter_add(\n",
    "            edge_weight, row, dim=0, dim_size=num_ent\n",
    "        )  # Summing number of weights of\n",
    "        # the edges, D = A + I\n",
    "        deg_inv = deg.pow(-0.5)  # D^{-0.5}\n",
    "        deg_inv[deg_inv == float(\"inf\")] = 0  # for numerical stability\n",
    "        norm = deg_inv[row] * edge_weight * deg_inv[col]  # Norm parameter D^{-0.5} *\n",
    "\n",
    "        return norm\n",
    "\n",
    "    def coalesce_quals(self, qual_embeddings, qual_index, num_edges, fill=0):\n",
    "        \"\"\"\n",
    "\n",
    "        before:\n",
    "            qualifier_emb      :   [a,b,c,d,e,f,g,......]               (here a,b,c ... are of 200 dim)\n",
    "            qual_index         :   [1,1,2,1,2,3,2,......]               (here 1,2,3 .. are edge index of Main COO)\n",
    "            edge_type          :   [0,0,0,0,0,0,0, .....]               (empty array of size num_edges)\n",
    "\n",
    "        After:\n",
    "            edge_type          :   [a+b+d,c+e+g,f ......]        (here each element in the list is of 200 dim)\n",
    "\n",
    "        :param qual_embeddings: shape of [1, N_QUALS]\n",
    "        :param qual_index: shape of [1, N_QUALS] which states which quals belong to which main relation from the index,\n",
    "            that is, all qual_embeddings that have the same index have to be summed up\n",
    "        :param num_edges: num_edges to return the appropriate tensor\n",
    "        :param fill: fill value for the output matrix - should be 0 for sum/concat and 1 for mul qual aggregation strat\n",
    "        :return: [1, N_EDGES]\n",
    "        \"\"\"\n",
    "\n",
    "        if self.p[\"STAREARGS\"][\"QUAL_N\"] == \"sum\":\n",
    "            output = scatter_add(qual_embeddings, qual_index, dim=0, dim_size=num_edges)\n",
    "        elif self.p[\"STAREARGS\"][\"QUAL_N\"] == \"mean\":\n",
    "            output = scatter_mean(\n",
    "                qual_embeddings, qual_index, dim=0, dim_size=num_edges\n",
    "            )\n",
    "\n",
    "        if fill != 0:\n",
    "            # by default scatter_ functions assign zeros to the output, so we assign them 1's for correct mult\n",
    "            mask = output.sum(dim=-1) == 0\n",
    "            output[mask] = fill\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}({}, {}, num_rels={})\".format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels, self.num_rels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    # \"DATASET\": \"wd50k\",\n",
    "    \"DEVICE\": \"cpu\",\n",
    "    \"EMBEDDING_DIM\": 200,\n",
    "    \"ENT_POS_FILTERED\": True,\n",
    "    \"EPOCHS\": 401,\n",
    "    \"EVAL_EVERY\": 5,\n",
    "    \"LEARNING_RATE\": 0.0001,\n",
    "    \"MAX_QPAIRS\": 15,\n",
    "    # \"MODEL_NAME\": \"stare_transformer\",\n",
    "    \"CORRUPTION_POSITIONS\": [0, 2],\n",
    "    \"SAVE\": False,\n",
    "    \"STATEMENT_LEN\": -1,\n",
    "    \"USE_TEST\": True,\n",
    "    # \"WANDB\": False,\n",
    "    \"LABEL_SMOOTHING\": 0.1,\n",
    "    \"SAMPLER_W_QUALIFIERS\": True,\n",
    "    \"OPTIMIZER\": \"adam\",\n",
    "    \"CLEANED_DATASET\": True,\n",
    "    \"GRAD_CLIPPING\": True,\n",
    "    \"LR_SCHEDULER\": True,\n",
    "    \"STAREARGS\": {\n",
    "        \"LAYERS\": 2,\n",
    "        \"N_BASES\": 0,\n",
    "        \"GCN_DIM\": 200,\n",
    "        \"GCN_DROP\": 0.1,\n",
    "        \"HID_DROP\": 0.3,\n",
    "        \"BIAS\": False,\n",
    "        \"OPN\": \"rotate\",\n",
    "        \"TRIPLE_QUAL_WEIGHT\": 0.8,\n",
    "        \"QUAL_AGGREGATE\": \"sum\",\n",
    "        \"QUAL_OPN\": \"rotate\",\n",
    "        \"QUAL_N\": \"sum\",\n",
    "        \"SUBBATCH\": 0,\n",
    "        \"QUAL_REPR\": \"sparse\",\n",
    "        \"ATTENTION\": False,\n",
    "        \"ATTENTION_HEADS\": 4,\n",
    "        \"ATTENTION_SLOPE\": 0.2,\n",
    "        \"ATTENTION_DROP\": 0.1,\n",
    "        \"HID_DROP2\": 0.1,\n",
    "        \"FEAT_DROP\": 0.3,\n",
    "        \"N_FILTERS\": 200,\n",
    "        \"KERNEL_SZ\": 7,\n",
    "        \"K_W\": 10,\n",
    "        \"K_H\": 20,\n",
    "        \"T_LAYERS\": 2,\n",
    "        \"T_N_HEADS\": 4,\n",
    "        \"T_HIDDEN\": 512,\n",
    "        \"POSITIONAL\": True,\n",
    "        \"POS_OPTION\": \"default\",\n",
    "        \"TIME\": False,\n",
    "        \"POOLING\": \"avg\",\n",
    "    },\n",
    "    \"NUM_ENTITIES\": len(entities),\n",
    "    \"NUM_RELATIONS\": len(relations),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = StarEConvLayer(4, 4, len(relations), act=torch.tanh, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def read_json(fname: str) -> dict:\n",
    "    \"\"\"Read json\"\"\"\n",
    "    with open(fname, \"r\") as stream:\n",
    "        return json.load(stream)\n",
    "\n",
    "\n",
    "def write_json(content: dict, fname: str) -> None:\n",
    "    \"\"\"Write json\"\"\"\n",
    "    with open(fname, \"w\") as stream:\n",
    "        json.dump(content, stream, indent=4, sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_index', 'edge_type', 'quals'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr = read_json(\"kg_graph_repr.json\")\n",
    "kg_graph_repr = {k: np.array(v) for k, v in kg_graph_repr.items()}\n",
    "kg_graph_repr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 380696)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr[\"edge_index\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380696,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr[\"edge_type\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 74866)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr[\"quals\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 163,  270,  471, ...,  648,  552, 1035])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr[\"edge_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   362,    362,    239, ...,    125,    125,    493],\n",
       "       [  1339,  17433,  10068, ...,  12219,  42900,  16382],\n",
       "       [     3,      3,      6, ..., 190310, 190324, 190342]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_graph_repr[\"quals\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
